name: In-Cluster Integration Tests

# ==============================================================================
# Reusable Workflow for In-Cluster Testing
# ==============================================================================
# This workflow can be called by other workflows to run integration tests
# in a Kubernetes cluster environment.
#
# Usage in other workflows:
#   jobs:
#     integration-tests:
#       uses: ./.github/workflows/in-cluster-test.yml
#       with:
#         test-path: "tests/integration/test_nats_events_integration.py"
#         test-name: "nats-events"
#         timeout: 600
# ==============================================================================

on:
  workflow_call:
    inputs:
      test-path:
        description: 'Path to test files (e.g., tests/integration/test_nats_events_integration.py)'
        required: true
        type: string
      test-name:
        description: 'Name for the test suite (used in job names and artifacts)'
        required: true
        type: string
      timeout:
        description: 'Timeout in seconds for test execution'
        required: false
        type: number
        default: 600
      image-tag:
        description: 'Docker image tag to use for testing'
        required: false
        type: string
        default: 'ci-test'
      namespace:
        description: 'Kubernetes namespace to use'
        required: false
        type: string
        default: 'intelligence-deepagents'
      use-real-llm:
        description: 'Whether to use real LLM APIs (requires secrets)'
        required: false
        type: boolean
        default: false
    secrets:
      OPENAI_API_KEY:
        required: false
      ANTHROPIC_API_KEY:
        required: false
      BOT_GITHUB_TOKEN:
        required: false
      BOT_GITHUB_USERNAME:
        required: false
      AWS_ROLE_ARN:
        required: false

permissions:
  id-token: write      # Required for AWS OIDC authentication
  contents: read
  pull-requests: write

jobs:
  in-cluster-tests:
    name: ${{ inputs.test-name }} Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
          token: ${{ secrets.BOT_GITHUB_TOKEN || github.token }}
      
      - name: Checkout zerotouch-platform
        uses: actions/checkout@v4
        with:
          repository: 'arun4infra/zerotouch-platform'
          path: 'zerotouch-platform'
          token: ${{ secrets.BOT_GITHUB_TOKEN || github.token }}
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ap-south-1
          mask-aws-account-id: true
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build test image
        run: |
          docker build -t deepagents-runtime:${{ inputs.image-tag }} .
      
      - name: Create Kind configuration
        run: |
          # Install kind if not available
          if ! command -v kind &> /dev/null; then
            echo "Installing kind..."
            curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
            chmod +x ./kind
            sudo mv ./kind /usr/local/bin/kind
          fi
          
          # Create Kind config that mounts zerotouch-platform subdirectory to /repo
          mkdir -p /tmp/kind
          cat > /tmp/kind/config.yaml << EOF
          kind: Cluster
          apiVersion: kind.x-k8s.io/v1alpha4
          name: zerotouch-preview
          nodes:
          - role: control-plane
            extraPortMappings:
            # NATS client port
            - containerPort: 30080
              hostPort: 4222
              protocol: TCP
            # PostgreSQL port
            - containerPort: 30432
              hostPort: 5432
              protocol: TCP
            # Dragonfly (Redis-compatible) port
            - containerPort: 30379
              hostPort: 6379
              protocol: TCP
            extraMounts:
            # Mount zerotouch-platform subdirectory for ArgoCD to sync from
            - hostPath: $(pwd)/zerotouch-platform
              containerPath: /repo
              readOnly: true
          EOF
      
      - name: Set up Kind cluster
        run: |
          kind create cluster --config /tmp/kind/config.yaml
          # Set kubectl context and label nodes
          kubectl config use-context kind-zerotouch-preview
          kubectl label nodes --all workload.bizmatters.dev/databases=true --overwrite
      
      - name: Load Docker image into Kind
        run: |
          kind load docker-image deepagents-runtime:${{ inputs.image-tag }} --name zerotouch-preview
      
      - name: Bootstrap platform
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          BOT_GITHUB_USERNAME: ${{ secrets.BOT_GITHUB_USERNAME }}
          BOT_GITHUB_TOKEN: ${{ secrets.BOT_GITHUB_TOKEN }}
          TENANTS_REPO_NAME: ${{ secrets.TENANTS_REPO_NAME }}
        run: |
          # Bootstrap the platform using the existing zerotouch-platform
          cd zerotouch-platform
          chmod +x scripts/bootstrap/01-master-bootstrap.sh
          ./scripts/bootstrap/01-master-bootstrap.sh --mode preview
      
      - name: Apply preview patches
        run: |
          # Apply preview environment patches for resource optimization
          chmod +x scripts/patches/00-apply-all-patches.sh
          ./scripts/patches/00-apply-all-patches.sh --force
      
      - name: Run pre-deploy diagnostics
        run: |
          # Run pre-deployment diagnostics
          chmod +x scripts/ci/pre-deploy-diagnostics.sh
          ./scripts/ci/pre-deploy-diagnostics.sh
      
      - name: Deploy service
        env:
          IMAGE_TAG: ${{ inputs.image-tag }}
          NAMESPACE: ${{ inputs.namespace }}
        run: |
          # Deploy the service using existing scripts
          ./scripts/ci/deploy.sh
      
      - name: Run post-deploy diagnostics
        env:
          NAMESPACE: ${{ inputs.namespace }}
        run: |
          # Run post-deployment diagnostics
          chmod +x scripts/ci/post-deploy-diagnostics.sh
          ./scripts/ci/post-deploy-diagnostics.sh ${{ inputs.namespace }} deepagents-runtime
      
      - name: Run in-cluster tests
        env:
          TEST_PATH: ${{ inputs.test-path }}
          TEST_NAME: ${{ inputs.test-name }}
          TIMEOUT: ${{ inputs.timeout }}
          NAMESPACE: ${{ inputs.namespace }}
          USE_REAL_LLM: ${{ inputs.use-real-llm }}
          USE_MOCK_LLM: ${{ inputs.use-real-llm == false && 'true' || 'false' }}
          MOCK_TIMEOUT: "60"
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          # Database credentials from K8s secrets (will be available in pod)
          POSTGRES_USER: ${{ secrets.POSTGRES_USER || 'postgres' }}
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD || 'postgres' }}
          POSTGRES_DB: ${{ secrets.POSTGRES_DB || 'langgraph_dev' }}
          DRAGONFLY_PASSWORD: ${{ secrets.DRAGONFLY_PASSWORD || 'dragonfly' }}
        run: |
          # Create and run test job using template
          export JOB_NAME="${{ inputs.test-name }}-$(date +%s)"
          
          # Substitute variables in template
          sed -e "s/{{JOB_NAME}}/$JOB_NAME/g" \
              -e "s/{{NAMESPACE}}/${{ inputs.namespace }}/g" \
              -e "s/{{IMAGE}}/deepagents-runtime:${{ inputs.image-tag }}/g" \
              -e "s|{{TEST_PATH}}|${{ inputs.test-path }}|g" \
              -e "s/{{TEST_NAME}}/${{ inputs.test-name }}/g" \
              -e "s/{{USE_MOCK_LLM}}/${USE_MOCK_LLM}/g" \
              -e "s/{{USE_REAL_LLM}}/${USE_REAL_LLM}/g" \
              -e "s/{{MOCK_TIMEOUT}}/${MOCK_TIMEOUT}/g" \
              scripts/ci/test-job-template.yaml > /tmp/test-job-base.yaml
          
          # Set API keys based on mock mode
          if [ "${USE_MOCK_LLM}" = "true" ]; then
            sed -e "s/{{OPENAI_API_KEY}}/mock-key-for-testing/g" \
                -e "s/{{ANTHROPIC_API_KEY}}/mock-key-for-testing/g" \
                /tmp/test-job-base.yaml > /tmp/test-job.yaml
          else
            sed -e "s/{{OPENAI_API_KEY}}/${OPENAI_API_KEY}/g" \
                -e "s/{{ANTHROPIC_API_KEY}}/${ANTHROPIC_API_KEY}/g" \
                /tmp/test-job-base.yaml > /tmp/test-job.yaml
          fi
          
          # Apply job and wait for completion
          kubectl apply -f /tmp/test-job.yaml
          
          echo "üöÄ Starting test job: $JOB_NAME"
          echo "‚è≥ Waiting for job to complete (timeout: ${{ inputs.timeout }}s)..."
          echo "üìä Namespace: ${{ inputs.namespace }}"
          echo ""
          
          # Enhanced wait with detailed progress logging
          ELAPSED=0
          POLL_INTERVAL=15
          TIMEOUT=${{ inputs.timeout }}
          
          while [ $ELAPSED -lt $TIMEOUT ]; do
            # Check job status
            JOB_STATUS=$(kubectl get job $JOB_NAME -n ${{ inputs.namespace }} -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
            JOB_FAILED=$(kubectl get job $JOB_NAME -n ${{ inputs.namespace }} -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null || echo "")
            
            if [ "$JOB_STATUS" = "True" ]; then
              echo "‚úÖ Job completed successfully after $((ELAPSED/60))m $((ELAPSED%60))s"
              break
            elif [ "$JOB_FAILED" = "True" ]; then
              echo "‚ùå Job failed after $((ELAPSED/60))m $((ELAPSED%60))s"
              break
            fi
            
            # Show progress every 30 seconds
            if [ $((ELAPSED % 30)) -eq 0 ] && [ $ELAPSED -gt 0 ]; then
              echo "‚è≥ Still waiting... ($((ELAPSED/60))m $((ELAPSED%60))s elapsed)"
              
              # Get pod status for progress indication
              POD_NAME=$(kubectl get pods -n ${{ inputs.namespace }} -l job-name=$JOB_NAME -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
              if [ -n "$POD_NAME" ]; then
                POD_PHASE=$(kubectl get pod $POD_NAME -n ${{ inputs.namespace }} -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                echo "   Pod: $POD_NAME ($POD_PHASE)"
                
                # Show container status
                CONTAINER_READY=$(kubectl get pod $POD_NAME -n ${{ inputs.namespace }} -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null || echo "false")
                CONTAINER_STATE=$(kubectl get pod $POD_NAME -n ${{ inputs.namespace }} -o jsonpath='{.status.containerStatuses[0].state}' 2>/dev/null | jq -r 'keys[0]' 2>/dev/null || echo "unknown")
                echo "   Container: ready=$CONTAINER_READY, state=$CONTAINER_STATE"
                
                # Show recent logs (last 3 lines) for progress indication
                if [ "$POD_PHASE" = "Running" ]; then
                  echo "   Recent logs:"
                  kubectl logs $POD_NAME -n ${{ inputs.namespace }} --tail=3 2>/dev/null | sed 's/^/     /' || echo "     (no logs yet)"
                fi
              else
                echo "   No pod found yet"
              fi
              echo ""
            fi
            
            sleep $POLL_INTERVAL
            ELAPSED=$((ELAPSED + POLL_INTERVAL))
          done
          
          # Final status check
          JOB_STATUS=$(kubectl get job $JOB_NAME -n ${{ inputs.namespace }} -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
          JOB_FAILED=$(kubectl get job $JOB_NAME -n ${{ inputs.namespace }} -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null || echo "")
          
          # Timeout case - comprehensive diagnostics
          if [ $ELAPSED -ge $TIMEOUT ] && [ "$JOB_STATUS" != "True" ]; then
            echo ""
            echo "üö® TIMEOUT: Job did not complete within $((TIMEOUT/60)) minutes"
            echo ""
            
            # Detailed job diagnostics
            echo "=== JOB DIAGNOSTICS ==="
            kubectl describe job $JOB_NAME -n ${{ inputs.namespace }} 2>/dev/null || echo "Could not describe job"
            echo ""
            
            # Pod diagnostics
            POD_NAME=$(kubectl get pods -n ${{ inputs.namespace }} -l job-name=$JOB_NAME -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            if [ -n "$POD_NAME" ]; then
              echo "=== POD DIAGNOSTICS ==="
              kubectl describe pod $POD_NAME -n ${{ inputs.namespace }} 2>/dev/null || echo "Could not describe pod"
              echo ""
              
              echo "=== POD EVENTS ==="
              kubectl get events -n ${{ inputs.namespace }} --field-selector involvedObject.name=$POD_NAME --sort-by='.lastTimestamp' 2>/dev/null || echo "Could not get events"
              echo ""
              
              echo "=== CONTAINER LOGS (LAST 100 LINES) ==="
              kubectl logs $POD_NAME -n ${{ inputs.namespace }} --tail=100 2>/dev/null || echo "Could not retrieve logs"
            else
              echo "‚ùå No pod found for job $JOB_NAME"
              echo ""
              echo "=== ALL PODS IN NAMESPACE ==="
              kubectl get pods -n ${{ inputs.namespace }} 2>/dev/null || echo "Could not list pods"
            fi
            
            echo ""
            echo "=== NAMESPACE EVENTS (RECENT WARNINGS) ==="
            kubectl get events -n ${{ inputs.namespace }} --sort-by='.lastTimestamp' --field-selector type=Warning 2>/dev/null | tail -10 || echo "Could not get namespace events"
            
            echo ""
            echo "=== DEBUG COMMANDS ==="
            echo "kubectl describe job $JOB_NAME -n ${{ inputs.namespace }}"
            echo "kubectl logs -l job-name=$JOB_NAME -n ${{ inputs.namespace }}"
            echo "kubectl get events -n ${{ inputs.namespace }} --sort-by='.lastTimestamp'"
            
            exit 1
          fi
          
          # Wait for job completion with better error handling (fallback)
          if ! kubectl wait --for=condition=complete --timeout=30s job/$JOB_NAME -n ${{ inputs.namespace }} 2>/dev/null; then
            echo "‚ùå Job did not complete within final check timeout"
            
            # Get job status for debugging
            echo ""
            echo "=== JOB STATUS DEBUG ==="
            kubectl describe job $JOB_NAME -n ${{ inputs.namespace }} || echo "Could not describe job"
            
            # Get pod status
            echo ""
            echo "=== POD STATUS DEBUG ==="
            POD_NAME=$(kubectl get pods -n ${{ inputs.namespace }} -l job-name=$JOB_NAME -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            if [ -n "$POD_NAME" ]; then
              echo "Pod: $POD_NAME"
              kubectl describe pod $POD_NAME -n ${{ inputs.namespace }} || echo "Could not describe pod"
              
              echo ""
              echo "=== POD LOGS (TIMEOUT CASE) ==="
              kubectl logs $POD_NAME -n ${{ inputs.namespace }} --tail=100 || echo "Could not retrieve logs"
            else
              echo "No pod found for job $JOB_NAME"
            fi
            
            exit 1
          fi
          
          # Check if job succeeded or failed
          JOB_STATUS=$(kubectl get job $JOB_NAME -n ${{ inputs.namespace }} -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "Unknown")
          JOB_FAILED=$(kubectl get job $JOB_NAME -n ${{ inputs.namespace }} -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null || echo "False")
          
          echo ""
          echo "=== JOB COMPLETION STATUS ==="
          echo "Job completion status: $JOB_STATUS"
          echo "Job failed status: $JOB_FAILED"
          
          # ALWAYS get pod logs for debugging (success or failure)
          POD_NAME=$(kubectl get pods -n ${{ inputs.namespace }} -l job-name=$JOB_NAME -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$POD_NAME" ]; then
            echo ""
            echo "=== COMPLETE TEST EXECUTION LOGS ==="
            echo "Pod: $POD_NAME"
            kubectl logs $POD_NAME -n ${{ inputs.namespace }} || echo "Could not retrieve logs"
            echo "=== END TEST EXECUTION LOGS ==="
          else
            echo "‚ùå No pod found for job $JOB_NAME - this indicates a serious issue"
            kubectl get pods -n ${{ inputs.namespace }} -l job-name=$JOB_NAME || echo "Could not list pods"
            exit 1
          fi
          
          # Check for job failure
          if [ "$JOB_FAILED" = "True" ]; then
            echo ""
            echo "‚ùå Test job failed!"
            
            # Additional debugging for failed jobs
            echo ""
            echo "=== FAILURE DIAGNOSTICS ==="
            kubectl describe job $JOB_NAME -n ${{ inputs.namespace }} || echo "Could not describe job"
            
            echo ""
            echo "=== POD FAILURE DETAILS ==="
            kubectl describe pod $POD_NAME -n ${{ inputs.namespace }} || echo "Could not describe pod"
            
            echo ""
            echo "=== RECENT EVENTS ==="
            kubectl get events -n ${{ inputs.namespace }} --sort-by='.lastTimestamp' --field-selector involvedObject.name=$POD_NAME | tail -10 || echo "Could not get events"
            
            exit 1
          elif [ "$JOB_STATUS" != "True" ]; then
            echo ""
            echo "‚ùå Test job did not complete successfully!"
            echo "Job status: Complete=$JOB_STATUS, Failed=$JOB_FAILED"
            
            # Additional debugging for incomplete jobs
            echo ""
            echo "=== INCOMPLETE JOB DIAGNOSTICS ==="
            kubectl describe job $JOB_NAME -n ${{ inputs.namespace }} || echo "Could not describe job"
            kubectl describe pod $POD_NAME -n ${{ inputs.namespace }} || echo "Could not describe pod"
            
            exit 1
          fi
          
          echo ""
          echo "‚úÖ Test job completed successfully"
      
      - name: Comment PR with test results
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const testName = '${{ inputs.test-name }}';
            
            const body = `## ‚úÖ ${testName} Tests Completed
            
            **Infrastructure:** In-Cluster Kubernetes (Kind + ArgoCD)
            **Test Path:** \`${{ inputs.test-path }}\`
            
            üéâ Tests completed successfully! Check the workflow logs for detailed results.
            
            [View workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
      
      - name: Cleanup
        if: always()
        run: |
          # Get logs from failed pods for debugging
          if kubectl get pods -n ${{ inputs.namespace }} -l test-suite=${{ inputs.test-name }} --field-selector=status.phase=Failed -o name | grep -q .; then
            echo "=== Failed Pod Logs ==="
            kubectl get pods -n ${{ inputs.namespace }} -l test-suite=${{ inputs.test-name }} --field-selector=status.phase=Failed -o name | while read pod; do
              echo "--- Logs for $pod ---"
              kubectl logs $pod -n ${{ inputs.namespace }} || true
            done
          fi
          
          # Clean up test jobs
          kubectl delete jobs -n ${{ inputs.namespace }} -l test-suite=${{ inputs.test-name }} --ignore-not-found=true
          
          # Clean up the Kind cluster
          kind delete cluster --name zerotouch-preview || true