"""
Helper functions for integration test event logging and artifact storage.

This module provides utilities for:
- Capturing and storing ALL streaming events
- Validating minimum event guarantees
- Generating execution summaries
- Saving artifacts to outputs/ directory

References:
    - agent-executor-event-example.md: Expected event structure
    - agent-executor-minimum-events.md: Minimum guaranteed event counts
"""

import json
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

import psycopg
import jsonschema


# ============================================================================
# CONSTANTS FROM agent-executor-minimum-events.md (DEEPAGENTS ARCHITECTURE)
# ============================================================================

# Critical guarantees (MUST PASS for any agent definition)
CRITICAL_GUARANTEES = {
    "on_llm_stream": 1,      # At least one LLM interaction (orchestrator)
    "on_state_update": 2,    # Initial state + at least one update
    "end": 1,                # Always exactly one end event
}

# Typical guarantees (SHOULD PASS for multi-specialist workflows)
TYPICAL_GUARANTEES = {
    "on_llm_stream": 6,      # Orchestrator + 5 specialists (min 1 token each)
    "on_state_update": 6,    # Initial + 5 specialist completions
    "end": 1,                # Always exactly one end event
}

# Note: deepagents DOES emit tool events for task tool and other tools
# Tool events are normal and expected (task, write_file, etc.)


# ============================================================================
# ARTIFACT STORAGE
# ============================================================================

# Global variable to store the current test run directory
_current_test_run_dir: Path = None


def get_output_dir() -> Path:
    """
    Get the outputs directory for test artifacts.
    
    Returns the base outputs directory (not the run-specific subdirectory).
    Use get_test_run_dir() to get the current run's directory.
    """
    output_dir = Path(__file__).parent / "outputs"
    output_dir.mkdir(exist_ok=True)
    return output_dir


def get_test_run_dir(test_id: str = None) -> Path:
    """
    Get or create the test run directory for the current test execution.
    
    All artifacts for a single test run are stored in a subdirectory
    named run_{timestamp}/ for easy organization and cleanup.
    
    Args:
        test_id: Optional test ID (timestamp). If not provided, generates a new one.
        
    Returns:
        Path to the test run directory
    """
    global _current_test_run_dir
    
    if _current_test_run_dir is None:
        if test_id is None:
            test_id = generate_test_id()
        
        output_dir = get_output_dir()
        _current_test_run_dir = output_dir / f"run_{test_id}"
        _current_test_run_dir.mkdir(exist_ok=True)
    
    return _current_test_run_dir


def reset_test_run_dir():
    """Reset the test run directory (called at start of each test)."""
    global _current_test_run_dir
    _current_test_run_dir = None


def save_artifact(filename: str, content: Any, as_json: bool = True) -> Path:
    """
    Save artifact to the current test run directory.
    
    Args:
        filename: Name of the file (without directory or timestamp prefix)
        content: Content to save (dict/list for JSON, str for text)
        as_json: If True, save as JSON with indentation
        
    Returns:
        Path to saved file
    """
    run_dir = get_test_run_dir()
    filepath = run_dir / filename
    
    if as_json:
        with open(filepath, 'w') as f:
            json.dump(content, f, indent=2, default=str)
    else:
        with open(filepath, 'w') as f:
            f.write(str(content))
    
    return filepath


def generate_test_id() -> str:
    """Generate unique test ID based on timestamp."""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def load_definition_with_files(definition_path: Path) -> Dict[str, Any]:
    """
    Load agent definition from JSON file and replace placeholders with content
    from corresponding files:
    - system_prompt: loaded from prompts/{node_id}.md
    - tool scripts: loaded from tools/{tool_name}.py
    - schema placeholders: injected from schema.json and schema_example.json
    
    This helper is specifically for test definitions to enable efficient debugging
    by keeping prompts and tool scripts in separate, readable files.
    
    Args:
        definition_path: Path to the definition.json file
        
    Returns:
        Dictionary with system_prompts and tool scripts loaded from files
        
    Raises:
        FileNotFoundError: If a prompt or tool file is missing
        ValueError: If definition structure is invalid
    """
    # Load the base definition
    with open(definition_path) as f:
        definition = json.load(f)
    
    # Get the prompts and tools directories (siblings to definition.json)
    prompts_dir = definition_path.parent / "prompts"
    tools_dir = definition_path.parent / "tools"
    
    if not prompts_dir.exists():
        raise FileNotFoundError(f"Prompts directory not found: {prompts_dir}")
    
    if not tools_dir.exists():
        raise FileNotFoundError(f"Tools directory not found: {tools_dir}")
    
    # Load schema files for injection into tool scripts
    schema_path = definition_path.parent / "schema.json"
    schema_example_path = definition_path.parent / "schema_example.json"
    
    schema_json = None
    schema_example_json = None
    
    if schema_path.exists():
        with open(schema_path) as f:
            schema_json = f.read()
    
    if schema_example_path.exists():
        with open(schema_example_path) as f:
            schema_example_json = f.read()
    
    # Process tool_definitions
    if "tool_definitions" in definition:
        for tool_def in definition["tool_definitions"]:
            if "runtime" not in tool_def or "script" not in tool_def["runtime"]:
                continue
            
            # Check if this is a placeholder that needs to be replaced
            script_value = tool_def["runtime"]["script"]
            if script_value.startswith("loaded from tools/") and script_value.endswith(" file"):
                tool_name = tool_def.get("name")
                if not tool_name:
                    raise ValueError(f"Tool definition missing 'name' field: {tool_def}")
                
                # Load the corresponding tool file
                tool_file = tools_dir / f"{tool_name}.py"
                if not tool_file.exists():
                    raise FileNotFoundError(
                        f"Tool file not found for tool '{tool_name}': {tool_file}"
                    )
                
                # Replace placeholder with file content
                with open(tool_file) as f:
                    script_content = f.read()
                
                # Inject schema content if placeholders exist
                if schema_json and "__SCHEMA_JSON__" in script_content:
                    # Parse and re-serialize to ensure valid Python dict literal
                    schema_dict = json.loads(schema_json)
                    script_content = script_content.replace(
                        "__SCHEMA_JSON__", 
                        json.dumps(schema_dict)
                    )
                
                if schema_example_json and "__SCHEMA_EXAMPLE_JSON__" in script_content:
                    # Use json.dumps to properly escape the string
                    script_content = script_content.replace(
                        "__SCHEMA_EXAMPLE_JSON__", 
                        json.dumps(schema_example_json.strip())
                    )
                
                tool_def["runtime"]["script"] = script_content
    
    # Process nodes (system_prompts)
    if "nodes" not in definition:
        raise ValueError("Definition must contain 'nodes' array")
    
    for node in definition["nodes"]:
        if "config" not in node or "system_prompt" not in node["config"]:
            continue
        
        # Check if this is a placeholder that needs to be replaced
        prompt_value = node["config"]["system_prompt"]
        if prompt_value == "loaded from file" or prompt_value.startswith("loaded from prompts/"):
            node_id = node.get("id")
            if not node_id:
                raise ValueError(f"Node missing 'id' field: {node}")
            
            # Load the corresponding prompt file
            prompt_file = prompts_dir / f"{node_id}.md"
            if not prompt_file.exists():
                raise FileNotFoundError(
                    f"Prompt file not found for node '{node_id}': {prompt_file}"
                )
            
            # Replace placeholder with file content
            with open(prompt_file) as f:
                node["config"]["system_prompt"] = f.read()
    
    return definition


# ============================================================================
# EVENT VALIDATION
# ============================================================================

def validate_minimum_events(events: List[Dict[str, Any]], use_typical: bool = True) -> Tuple[bool, List[str]]:
    """
    Validate minimum guaranteed event counts for deepagents architecture.
    
    Args:
        events: List of streaming events
        use_typical: If True, use typical guarantees; if False, use critical guarantees
        
    Returns:
        Tuple of (is_valid, list of error messages)
    """
    errors = []
    
    # Count events by type
    event_counts = {}
    for event in events:
        event_type = event.get("event_type")
        event_counts[event_type] = event_counts.get(event_type, 0) + 1
    
    # Choose validation level
    guarantees = TYPICAL_GUARANTEES if use_typical else CRITICAL_GUARANTEES
    
    # Validate minimum guarantees
    for event_type, min_count in guarantees.items():
        actual_count = event_counts.get(event_type, 0)
        
        if event_type == "end":
            # end event must be exactly 1
            if actual_count != min_count:
                errors.append(
                    f"Expected exactly {min_count} '{event_type}' event, got {actual_count}"
                )
        else:
            # Other events must be >= minimum
            if actual_count < min_count:
                errors.append(
                    f"Expected at least {min_count} '{event_type}' events, got {actual_count}"
                )
    
    # Note: Tool events (on_tool_start, on_tool_end) are normal and expected
    # No forbidden events validation needed
    
    return len(errors) == 0, errors


# In test_helpers.py

def validate_specialist_order(events: List[Dict[str, Any]]) -> Tuple[bool, List[str]]:
    """
    Validate specialist execution order by inspecting AIMessage tool calls
    from the final state update.
    """
    errors = []
    # Find the last `on_state_update` event before the `end` event.
    last_state_update = next((e for e in reversed(events) if e.get("event_type") == "on_state_update"), None)

    if not last_state_update:
        errors.append("Validation Error: No 'on_state_update' events found to validate specialist order.")
        return False, errors

    # The messages are serialized as a string inside the 'data' payload.
    messages_str = last_state_update.get("data", {}).get("messages", "[]")
    
    try:
        # A bit complex: the string is a repr of a list, not clean JSON. We can use ast.
        import ast
        messages = ast.literal_eval(messages_str)
    except (ValueError, SyntaxError):
        errors.append("Failed to parse messages from state update event.")
        return False, errors

    # Extract the 'subagent_type' from each 'task' tool call in AIMessages
    actual_order = []
    for msg in messages:
        if msg.startswith("AIMessage") and "'name': 'task'" in msg:
            # Simple string parsing to find the subagent_type
            try:
                args_part = msg.split("'args': {")[1].split("}")[0]
                if "'subagent_type': '" in args_part:
                    subagent = args_part.split("'subagent_type': '")[1].split("'")[0]
                    actual_order.append(subagent.replace(" ", "-").lower())
            except IndexError:
                continue # Malformed tool call string

    # The log shows a restart, so we expect two sequences. We check the last one.
    expected_order = [
        "guardrail-agent",
        "impact-analysis-agent",
        "workflow-spec-agent",
        "agent-spec-agent",
        "multi-agent-compiler-agent",
    ]
    
    # Check if the expected order is a subsequence of the actual order
    # This handles restarts gracefully.
    actual_order_str = " ".join(actual_order)
    expected_order_str = " ".join(expected_order)

    if expected_order_str not in actual_order_str:
        errors.append(f"Specialist execution order is incorrect.")
        errors.append(f"  Expected subsequence: {expected_order}")
        errors.append(f"  Actual full order:    {actual_order}")

    return len(errors) == 0, errors


def validate_event_structure(events: List[Dict[str, Any]]) -> Tuple[bool, List[str]]:
    """
    Validate event structure and flow for deepagents architecture.
    
    Args:
        events: List of streaming events
        
    Returns:
        Tuple of (is_valid, list of error messages)
    """
    errors = []
    
    if not events:
        errors.append("No events captured")
        return False, errors
    
    # Check that all events have required structure
    for i, event in enumerate(events):
        if not isinstance(event, dict):
            errors.append(f"Event {i}: Expected dict, got {type(event)}")
            continue
        
        if "event_type" not in event:
            errors.append(f"Event {i}: Missing 'event_type' field")
        
        if "data" not in event:
            errors.append(f"Event {i}: Missing 'data' field")
    
    # Check that end event is last (if present)
    end_events = [i for i, event in enumerate(events) if event.get("event_type") == "end"]
    if end_events:
        last_end = max(end_events)
        if last_end != len(events) - 1:
            errors.append(f"'end' event should be last, but found at position {last_end} of {len(events)}")
    
    # Tool events are expected and normal (task tool, write_file, etc.)
    # No validation needed here - tool events are part of normal operation
    
    return len(errors) == 0, errors


# ============================================================================
# CHECKPOINT EXTRACTION
# ============================================================================

def extract_checkpoints(
    postgres_connection: psycopg.Connection,
    job_id: str
) -> List[Dict[str, Any]]:
    """
    Extract checkpoints from PostgreSQL for a given job_id.
    
    Args:
        postgres_connection: PostgreSQL connection
        job_id: Job ID (thread_id)
        
    Returns:
        List of checkpoint dictionaries
    """
    with postgres_connection.cursor() as cur:
        cur.execute("""
            SELECT thread_id, checkpoint_id, checkpoint, metadata
            FROM checkpoints
            WHERE thread_id = %s
            ORDER BY checkpoint_id
        """, (job_id,))
        
        rows = cur.fetchall()
    
    checkpoints = []
    for row in rows:
        thread_id, checkpoint_id, checkpoint_data, metadata = row
        checkpoints.append({
            "thread_id": thread_id,
            "checkpoint_id": checkpoint_id,
            "checkpoint": checkpoint_data,
            "metadata": metadata
        })
    
    return checkpoints


# ============================================================================
# SPECIALIST TIMELINE EXTRACTION
# ============================================================================

def extract_specialist_timeline(events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Extracts a more accurate specialist timeline by pairing AIMessage tool calls
    with their resulting ToolMessage.
    """
    timeline = []
    # Find the last `on_state_update` event before the `end` event.
    last_state_update = next((e for e in reversed(events) if e.get("event_type") == "on_state_update"), None)
    if not last_state_update:
        return []

    messages_str = last_state_update.get("data", {}).get("messages", "[]")
    
    try:
        import ast
        messages = ast.literal_eval(messages_str)
    except (ValueError, SyntaxError):
        return []

    tool_calls = {} # Store AI tool calls by their ID

    for msg_str in messages:
        if msg_str.startswith("AIMessage"):
            try:
                # Extract tool call ID and agent type
                tool_call_id = msg_str.split("'id': '")[1].split("'")[0]
                if "'subagent_type': '" in msg_str:
                    subagent = msg_str.split("'subagent_type': '")[1].split("'")[0]
                    tool_calls[tool_call_id] = {"specialist": subagent, "start_timestamp": "N/A"}
            except IndexError:
                continue
        
        elif msg_str.startswith("ToolMessage"):
            try:
                # Match tool message back to the AI call
                tool_call_id = msg_str.split("tool_call_id='")[1].split("'")[0]
                if tool_call_id in tool_calls:
                    # For this test, we don't have timestamps in messages, so duration is unknown
                    tool_calls[tool_call_id]["duration_ms"] = "Unknown"
                    tool_calls[tool_call_id]["duration_s"] = "Unknown"
                    timeline.append(tool_calls[tool_call_id])
            except IndexError:
                continue

    return timeline


# ============================================================================
# WORKFLOW RESULT VALIDATION
# ============================================================================

def validate_workflow_result(result: Dict[str, Any], checkpoints: List[Dict[str, Any]]) -> Tuple[bool, List[str]]:
    """
    Validate that a workflow execution result is successful and complete.
    
    This function checks for:
    1. No HALT errors in the output
    2. Status is completed
    3. Output indicates successful completion
    4. Checkpoints exist (indicating graph execution occurred)
    
    Args:
        result: The result dictionary from CloudEvent data
        checkpoints: Required list of checkpoints from PostgreSQL database
        
    Returns:
        Tuple of (is_valid, list of error messages)
    """
    errors = []
    
    # Check 1: Validate status
    status = result.get("status")
    if status != "completed":
        errors.append(f"Result status is '{status}', expected 'completed'")
    
    # Check 2: Check for HALT errors
    output = result.get("output", "")
    if output.startswith("HALT:"):
        errors.append(f"Workflow halted with error: {output}")
        return False, errors
    
    # Check 3: Validate output indicates success
    if not output:
        errors.append("No output message found in result")
    elif "successfully" not in output.lower() and "completed" not in output.lower():
        errors.append(f"Output does not indicate successful completion: {output[:100]}...")
    
    # Check 4: Validate workflow execution completed properly using checkpoints
    # Checkpoints are required to validate that graph execution actually occurred
    if len(checkpoints) == 0:
        errors.append("No checkpoints found - graph execution may not have started")
    
    # Note: The actual validation of workflow success (definition.json generation, etc.)
    # should be done by examining the Redis streaming events in the test, not here.
    # This function only validates the basic result structure and checkpoint existence.
    
    return len(errors) == 0, errors


def validate_redis_artifacts(events: List[Dict[str, Any]], job_id: str) -> Tuple[bool, List[str]]:
    """
    Validate that required file system artifacts were generated and emitted in Redis streaming events.
    
    This function examines the final on_state_update event (just before the end event) to verify 
    that the multi-agent workflow successfully generated all expected specification files and 
    the final definition.json.
    
    Args:
        events: List of Redis streaming events
        job_id: Job ID for context in error messages
        
    Returns:
        Tuple of (is_valid, list of error messages)
    """
    errors = []
    
    if not events:
        errors.append("No Redis events available for artifact validation")
        return False, errors
    
    # Find the final on_state_update event (just before the end event)
    final_state_update = None
    for i in range(len(events) - 1, -1, -1):  # Search backwards
        event = events[i]
        if event.get("event_type") == "on_state_update":
            final_state_update = event
            break
    
    if not final_state_update:
        errors.append("No final on_state_update event found in Redis stream")
        return False, errors
    
    # Extract files from the final state update
    event_data = final_state_update.get("data", {})
    files_state = event_data.get("files", {})
    
    if not isinstance(files_state, dict):
        errors.append(f"Files state is not a dictionary, got: {type(files_state)}")
        return False, errors
    
    if not files_state:
        errors.append("No files found in final state update event")
        return False, errors
    
    # Define required artifacts based on the Builder Agent workflow
    required_files = [
        "/THE_SPEC/constitution.md",
        "/THE_SPEC/plan.md", 
        "/THE_SPEC/requirements.md",
        "/definition.json"
    ]
    
    # Validate each required file exists in the files state
    missing_files = []
    for file_path in required_files:
        if file_path not in files_state:
            missing_files.append(file_path)
    
    if missing_files:
        errors.append(f"Missing required artifacts in Redis event files: {missing_files}")
    
    # Additional validation: Check that files have content
    empty_files = []
    for file_path in required_files:
        if file_path in files_state:
            file_data = files_state[file_path]
            # File data structure: {"content": ["line1", "line2", ...], "created_at": "...", "modified_at": "..."}
            if isinstance(file_data, dict):
                content = file_data.get("content", [])
                if not content or (isinstance(content, list) and len(content) == 0):
                    empty_files.append(file_path)
            elif not file_data:  # Handle other formats
                empty_files.append(file_path)
    
    if empty_files:
        errors.append(f"Required artifacts exist but are empty: {empty_files}")
    
    # ================================================================
    # SCHEMA VALIDATION: Validate definition.json against schema
    # ================================================================
    if "/definition.json" in files_state and not errors:  # Only validate if file exists and no previous errors
        try:
            # Load the schema
            schema_path = Path(__file__).parent.parent / "mock" / "schema.json"
            if not schema_path.exists():
                errors.append(f"Schema file not found: {schema_path}")
            else:
                with open(schema_path, 'r') as f:
                    schema = json.load(f)
                
                # Extract definition.json content from Redis event
                definition_file_data = files_state["/definition.json"]
                
                # Handle file data format from Redis events
                if isinstance(definition_file_data, dict):
                    content = definition_file_data.get("content", [])
                    if isinstance(content, list) and content:
                        # Join content lines if it's a list
                        definition_content = "".join(content)
                    else:
                        definition_content = str(content)
                else:
                    definition_content = str(definition_file_data)
                
                # Parse JSON content
                try:
                    definition_json = json.loads(definition_content)
                except json.JSONDecodeError as e:
                    errors.append(f"definition.json contains invalid JSON: {e}")
                    return len(errors) == 0, errors
                
                # Validate against schema
                try:
                    jsonschema.validate(instance=definition_json, schema=schema)
                except jsonschema.ValidationError as e:
                    errors.append(f"definition.json schema validation failed: {e.message}")
                except jsonschema.SchemaError as e:
                    errors.append(f"Invalid schema file: {e.message}")
                    
        except Exception as e:
            errors.append(f"Unexpected error during schema validation: {e}")
    
    return len(errors) == 0, errors


def validate_checkpoint_artifacts(checkpoints: List[Dict[str, Any]], job_id: str) -> Tuple[bool, List[str]]:
    """
    DEPRECATED: Use validate_redis_artifacts instead.
    
    This function is kept for backward compatibility but should not be used.
    Artifacts are now validated from Redis streaming events, not PostgreSQL checkpoints.
    """
    # For backward compatibility, just check that checkpoints exist
    if not checkpoints:
        return False, ["No checkpoints found - workflow execution may not have started"]
    
    # Return success since actual validation is done via Redis events
    return True, []


# ============================================================================
# SUMMARY GENERATION
# ============================================================================

def generate_execution_summary(
    events: List[Dict[str, Any]],
    checkpoints: List[Dict[str, Any]],
    specialist_timeline: List[Dict[str, Any]],
    cloudevent: Dict[str, Any],
    total_duration_s: float
) -> str:
    """
    Generate human-readable execution summary.
    
    Args:
        events: List of streaming events
        checkpoints: List of checkpoints
        specialist_timeline: Specialist execution timeline
        cloudevent: Final CloudEvent
        total_duration_s: Total execution duration in seconds
        
    Returns:
        Formatted summary string
    """
    # Count events by type
    event_counts = {}
    for event in events:
        event_type = event.get("event_type")
        event_counts[event_type] = event_counts.get(event_type, 0) + 1
    
    # Calculate percentages
    total_events = len(events)
    event_breakdown = []
    for event_type, count in sorted(event_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_events * 100) if total_events > 0 else 0
        event_breakdown.append(f"  {event_type:20s} {count:5d} ({percentage:5.1f}%)")
    
    # Build summary
    summary_lines = [
        "=" * 80,
        "EXECUTION SUMMARY",
        "=" * 80,
        f"Total Duration: {total_duration_s:.1f}s",
        f"Total Events: {total_events}",
        "",
        "Event Type Breakdown:",
        *event_breakdown,
        "",
        "State Update Timeline:",
    ]
    
    for spec in specialist_timeline:
        summary_lines.append(
            f"  Step {spec['step']}: {spec['event_type']} at {spec['timestamp']}"
        )
    
    summary_lines.extend([
        "",
        f"PostgreSQL Checkpoints: {len(checkpoints)}",
        f"CloudEvents Emitted: 1 ({cloudevent.get('type', 'unknown')})",
        "",
        "Agent Definition Summary:",
        f"  Nodes: {len(cloudevent.get('data', {}).get('result', {}).get('final_state', {}).get('definition', {}).get('nodes', []))}",
        f"  Status: {cloudevent.get('data', {}).get('result', {}).get('status', 'unknown')}",
        "=" * 80,
    ])
    
    return "\n".join(summary_lines)


def generate_checkpoint_summary(checkpoints: List[Dict[str, Any]]) -> str:
    """
    Generate checkpoint summary.
    
    Args:
        checkpoints: List of checkpoints
        
    Returns:
        Formatted checkpoint summary string
    """
    if not checkpoints:
        return "No checkpoints found"
    
    thread_id = checkpoints[0]["thread_id"]
    
    summary_lines = [
        "=" * 80,
        "POSTGRESQL CHECKPOINTS",
        "=" * 80,
        f"Total: {len(checkpoints)} checkpoints for thread_id: {thread_id}",
        "",
    ]
    
    # Only show checkpoint timeline if there are a reasonable number of checkpoints
    if len(checkpoints) <= 20:
        summary_lines.append("Checkpoint Timeline:")
        for i, checkpoint in enumerate(checkpoints, 1):
            checkpoint_id = checkpoint["checkpoint_id"]
            summary_lines.append(f"{i}. {checkpoint_id}")
    else:
        # For large numbers of checkpoints, show first 5 and last 5
        summary_lines.extend([
            "Checkpoint Timeline (showing first 5 and last 5):",
            "First 5 checkpoints:"
        ])
        for i in range(min(5, len(checkpoints))):
            checkpoint_id = checkpoints[i]["checkpoint_id"]
            summary_lines.append(f"{i+1}. {checkpoint_id}")
        
        if len(checkpoints) > 10:
            summary_lines.append("...")
            summary_lines.append("Last 5 checkpoints:")
            for i in range(max(0, len(checkpoints) - 5), len(checkpoints)):
                checkpoint_id = checkpoints[i]["checkpoint_id"]
                summary_lines.append(f"{i+1}. {checkpoint_id}")
    
    summary_lines.extend([
        "",
        "✓ All checkpoints use correct thread_id (job_id)",
        "✓ Checkpoints saved after each specialist",
        "=" * 80,
    ])
    
    return "\n".join(summary_lines)


def generate_cloudevent_summary(cloudevent: Dict[str, Any]) -> str:
    """
    Generate CloudEvent summary.
    
    Args:
        cloudevent: CloudEvent dictionary
        
    Returns:
        Formatted CloudEvent summary string
    """
    data = cloudevent.get("data", {})
    result = data.get("result", {})
    final_state = result.get("final_state", {})
    definition = final_state.get("definition", {})
    
    nodes = definition.get("nodes", [])
    edges = definition.get("edges", [])
    tool_definitions = definition.get("tool_definitions", [])
    
    summary_lines = [
        "=" * 80,
        "CLOUDEVENT RESULT",
        "=" * 80,
        f"Type: {cloudevent.get('type')}",
        f"Subject: {cloudevent.get('subject')}",
        f"Trace ID: {cloudevent.get('traceparent', 'N/A').split('-')[1] if cloudevent.get('traceparent') else 'N/A'}",
        "",
        "Result Summary:",
        f"  Status: {result.get('status')}",
        f"  Output: {result.get('output', 'N/A')[:80]}...",
        "",
        "Agent Definition:",
        f"  Nodes: {len(nodes)} ({', '.join([n.get('id', 'unknown') for n in nodes])})",
        f"  Edges: {len(edges)}",
        f"  Tool Definitions: {len(tool_definitions)}",
        "",
        "✓ CloudEvent emitted successfully",
        "✓ W3C Trace Context propagated",
        "=" * 80,
    ]
    
    return "\n".join(summary_lines)


# ============================================================================
# FILE EXTRACTION FROM EVENTS
# ============================================================================

def extract_and_save_generated_files(events: List[Dict[str, Any]], run_dir: Path = None) -> Dict[str, str]:
    """
    Extract all generated files from streaming events and save them to a 'files/' subdirectory.
    
    This function extracts files from the final on_state_update event's 'files' field,
    which contains all files created during the agent execution.
    
    Args:
        events: List of streaming events from the agent execution
        run_dir: Optional path to the test run directory. If not provided, uses get_test_run_dir()
        
    Returns:
        Dictionary mapping file paths to their content
    """
    if run_dir is None:
        run_dir = get_test_run_dir()
    
    # Create files subdirectory
    files_dir = run_dir / "files"
    files_dir.mkdir(exist_ok=True)
    
    extracted_files = {}
    
    # Find the last on_state_update event which contains the final files state
    last_state_update = None
    for event in reversed(events):
        if event.get("event_type") == "on_state_update":
            last_state_update = event
            break
    
    if last_state_update:
        data = last_state_update.get("data", {})
        files_state = data.get("files", {})
        
        # Extract all files from the state
        for file_path, file_data in files_state.items():
            if isinstance(file_data, dict) and "content" in file_data:
                content = file_data["content"]
                # Content is stored as a list of lines
                if isinstance(content, list):
                    content = "\n".join(content)
                extracted_files[file_path] = content
    
    # Save each file to the files/ subdirectory
    for file_path, content in extracted_files.items():
        # Convert absolute path to relative filename
        # /THE_SPEC/plan.md -> THE_SPEC_plan.md
        safe_filename = file_path.lstrip("/").replace("/", "_")
        
        # Keep the original extension if it exists
        if not any(safe_filename.endswith(ext) for ext in [".md", ".json", ".py", ".txt", ".yaml", ".yml"]):
            safe_filename += ".txt"
        
        output_path = files_dir / safe_filename
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"[FILES] Saved: {output_path.name}")
        except Exception as e:
            print(f"[FILES] Error saving {safe_filename}: {e}")
    
    # Also save a manifest of all extracted files with metadata
    manifest = {
        "total_files": len(extracted_files),
        "files": []
    }
    
    # Add file metadata to manifest
    if last_state_update:
        files_state = last_state_update.get("data", {}).get("files", {})
        for file_path, file_data in files_state.items():
            if isinstance(file_data, dict):
                manifest["files"].append({
                    "path": file_path,
                    "created_at": file_data.get("created_at"),
                    "modified_at": file_data.get("modified_at"),
                    "size": len(file_data.get("content", [])) if isinstance(file_data.get("content"), list) else len(str(file_data.get("content", "")))
                })
    
    manifest_path = files_dir / "_manifest.json"
    with open(manifest_path, 'w', encoding='utf-8') as f:
        json.dump(manifest, f, indent=2)
    
    print(f"[FILES] Extracted {len(extracted_files)} files to {files_dir}")
    
    return extracted_files
